{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NutriNudge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PIP installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain_community\n",
    "%pip install langchain\n",
    "%pip install accelerate\n",
    "%pip install qdrant_client\n",
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\navee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-] Loaded configurations\n",
      "[-] Embedding model initialised\n",
      "[-] Qdrant Vector Database client started\n",
      "[-] Vector embeddings obtained\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import CTransformers\n",
    "from langchain.chains import RetrievalQA\n",
    "from accelerate import Accelerator\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "import json\n",
    "import time\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "allergens_set = {'Soybeans', 'Eggs', 'Celery', 'Pine nuts', 'Peanuts', 'Almonds', 'Shellfish', 'Pork', 'Nuts', 'Anchovies',\n",
    "                 'Mustard', 'Milk', 'Coconut', 'Strawberries', 'Alcohol', 'Chicken', 'Ghee', 'Fish', 'Cocoa', 'Wheat', 'Oats', 'Dairy', 'Rice'}\n",
    "\n",
    "# Importing config file\n",
    "CONFIG = None\n",
    "with open(\".\\\\..\\\\config.json\") as f:\n",
    "    CONFIG = json.load(f)\n",
    "print(\"[-] Loaded configurations\")\n",
    "\n",
    "# Tokenizer Details\n",
    "# model_name = CONFIG[\"token-model-name\"]\n",
    "model_kwargs = {\"device\": 'cuda'}\n",
    "encode_kwargs = {\"normalize_embeddings\": False}\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_kwargs=model_kwargs,\n",
    "    # model_name = model_name,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "print(\"[-] Embedding model initialised\")\n",
    "\n",
    "hist = \"\"\n",
    "\n",
    "url = CONFIG[\"vector-db-url\"]\n",
    "api_key = CONFIG[\"vector-api-key\"]\n",
    "collection_name = CONFIG[\"vector-collection-name\"]\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=url,\n",
    "    api_key=api_key,\n",
    "    # prefer_grpc=True\n",
    ")\n",
    "\n",
    "print(\"[-] Qdrant Vector Database client started\")\n",
    "\n",
    "db = Qdrant(\n",
    "    client=client,\n",
    "    embeddings=embeddings,\n",
    "    collection_name=collection_name\n",
    ")\n",
    "\n",
    "print(\"[-] Vector embeddings obtained\")\n",
    "\n",
    "\n",
    "def demo_vector_select():\n",
    "    query = \"List the products you have...\"\n",
    "    docs = db.similarity_search_with_score(query=query, k=5)\n",
    "    doc, score = docs[0]\n",
    "    print(\"##########################################################\")\n",
    "    print({\"score\": score, \"content\": doc.page_content, \"metadata\": doc.metadata})\n",
    "    print(\"##########################################################\")\n",
    "\n",
    "\n",
    "cpt = \"\"\"\n",
    "<s>\n",
    "[INST]\n",
    "## System\n",
    "You are an AI assistant for NutriNudge, an online retail shop specializing in food products. Your role is to offer a personalized shopping experience by suggesting suitable products for users, taking into account their specific allergies. You must filter out products that contain any allergens listed by the user.\n",
    "The user specific allergens are given in \"User Allergies\" section. The products present are provided in the \"Product Info\" section.\n",
    "\n",
    "## Product Info: \n",
    "{context}\n",
    "\n",
    "------------------------------------------------------------------\n",
    "## User Allergies:\n",
    "$user_allergies$\n",
    "\n",
    "Below provided is the list of you previous conversations, use it to give personilized reponses.\n",
    "## Chat History:\n",
    "$chat_history$\n",
    "------------------------------------------------------------------\n",
    "## Question: \n",
    "{question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "[/INST]\n",
    "## Helpful answer:\n",
    "</s>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def update_prompt(user_allergies):\n",
    "    global cpt\n",
    "    cpt = cpt.replace(\"$chat_history$\", hist)\n",
    "    print(\"[-] History added to prompt\")\n",
    "    cpt = cpt.replace(\"$user_allergies$\", str(user_allergies))\n",
    "    print(\"[-] Allergies intimated to model\")\n",
    "    prompt = PromptTemplate(template=cpt,\n",
    "                            input_variables=['context', 'question'])\n",
    "    print(\"[-] Prompt formatted\")\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def retrieval_qa_chain(llm, prompt, db, allergens):\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                           chain_type='stuff',\n",
    "                                           retriever=db.as_retriever(\n",
    "                                               search_kwargs={'k': 10,\n",
    "                                                              \"filter\": models.Filter(\n",
    "                                                                  must_not=[\n",
    "                                                                      models.FieldCondition(\n",
    "                                                                          key=\"Ingredients\",\n",
    "                                                                          match=models.MatchAny(\n",
    "                                                                              any=allergens,\n",
    "                                                                          )\n",
    "                                                                      )\n",
    "                                                                  ]\n",
    "                                                              ), }),\n",
    "                                           return_source_documents=True,\n",
    "                                           chain_type_kwargs={\n",
    "                                               'prompt': prompt},\n",
    "                                           )\n",
    "    print(\"[-] QA chain initialised\")\n",
    "\n",
    "    return qa_chain\n",
    "\n",
    "\n",
    "def load_llm():\n",
    "    conf = {\"max_new_tokens\": 1024, \"top_k\": 5, \"top_p\": 0.80,\n",
    "            \"context_length\": 5096, \"gpu_layers\": 10}\n",
    "    llm = CTransformers(\n",
    "        model=CONFIG[\"model-link\"],\n",
    "        model_type=CONFIG[\"model-name\"],\n",
    "        temperature=0.3,\n",
    "        config=conf\n",
    "    )\n",
    "    llm, conf = accelerator.prepare(llm, conf)\n",
    "    # llm = AutoModelForCausalLM.from_pretrained(CONFIG[\"model-link\"], model_type=\"mistral\", gpu_layers=0, config=conf, local_files_only=True)\n",
    "    print(\"[-] LLM loaded\")\n",
    "    return llm\n",
    "\n",
    "\n",
    "def qa_bot(user_allergies):\n",
    "    llm = load_llm()\n",
    "    qa_prompt = update_prompt(user_allergies)\n",
    "    qa = retrieval_qa_chain(llm, qa_prompt, db, user_allergies)\n",
    "    print(\"[-] Chatbot is online\")\n",
    "    return qa\n",
    "\n",
    "\n",
    "def final_result(query, user_allergies):\n",
    "    global hist\n",
    "    qa_result = qa_bot(user_allergies)\n",
    "    print(\"[.] Generating recommendations for the query: \"+query)\n",
    "    res = qa_result.invoke({'query': query})\n",
    "    print(\"[.] Response obtained\")\n",
    "    answer = res[\"result\"]\n",
    "    sources = res[\"source_documents\"]\n",
    "    with open(\"res.json\", \"w\") as f:\n",
    "        f.write(str(res))\n",
    "    with open(\"answer.json\", \"w\") as f:\n",
    "        f.write(str(answer))\n",
    "    with open(\"sources.json\", \"w\") as f:\n",
    "        print(\"sources:\\n\", sources)\n",
    "        f.write(str(sources))\n",
    "    hist += query + \":\" + answer + \"\\n\"\n",
    "    rfcnc = \"Look at the below suggested results...\"\n",
    "    components = []\n",
    "    if sources:\n",
    "        for source in sources:\n",
    "            components.append({\n",
    "                \"title\": source.metadata[\"Food Product\"],\n",
    "                \"ingredients\": source.metadata[\"Ingredients\"]\n",
    "            })\n",
    "    else:\n",
    "        rfcnc = \"\"\n",
    "    response = answer+\"\\n\"+rfcnc\n",
    "    return {\"response\": response, \"components\": components}\n",
    "\n",
    "\n",
    "def search_product(prompt, user_allergies):\n",
    "    global cpt\n",
    "    st = time.time()\n",
    "    res = final_result(prompt, user_allergies)\n",
    "    ed = time.time()\n",
    "    print(\"&&&&\"*10)\n",
    "    print(cpt)\n",
    "    print(\"====\"*10)\n",
    "    print(res)\n",
    "    print(\"####\"*10)\n",
    "    print(\"Time Taken:\", ed-st, \"secs\")\n",
    "    print(\"!!!!\"*10)\n",
    "    print(\"[->] Responding...\", res)\n",
    "    print(\"[$$] Time Taken:\", ed-st, \"secs\")\n",
    "    return res\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # demo_vector_select()\n",
    "    search_product(\"i want cookie\", [\"almond\", \"chocolate\"])\n",
    "    search_product(\"i want lemon\", [])\n",
    "    search_product(\"Hey do you guys offer refund?\", [\"chocolate\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import FlaxAutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME_OR_PATH = \"flax-community/t5-recipe-generation\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "model = FlaxAutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "\n",
    "prefix = \"items: \"\n",
    "# generation_kwargs = {\n",
    "#     \"max_length\": 512,\n",
    "#     \"min_length\": 64,\n",
    "#     \"no_repeat_ngram_size\": 3,\n",
    "#     \"early_stopping\": True,\n",
    "#     \"num_beams\": 5,\n",
    "#     \"length_penalty\": 1.5,\n",
    "# }\n",
    "generation_kwargs = {\n",
    "    \"max_length\": 512,\n",
    "    \"min_length\": 64,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    \"do_sample\": True,\n",
    "    \"top_k\": 60,\n",
    "    \"top_p\": 0.95\n",
    "}\n",
    "\n",
    "\n",
    "special_tokens = tokenizer.all_special_tokens\n",
    "tokens_map = {\n",
    "    \"<sep>\": \"--\",\n",
    "    \"<section>\": \"\\n\"\n",
    "}\n",
    "\n",
    "\n",
    "def skip_special_tokens(text, special_tokens):\n",
    "    for token in special_tokens:\n",
    "        text = text.replace(token, \"\")\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def target_postprocessing(texts, special_tokens):\n",
    "    if not isinstance(texts, list):\n",
    "        texts = [texts]\n",
    "\n",
    "    new_texts = []\n",
    "    for text in texts:\n",
    "        text = skip_special_tokens(text, special_tokens)\n",
    "\n",
    "        for k, v in tokens_map.items():\n",
    "            text = text.replace(k, v)\n",
    "\n",
    "        new_texts.append(text)\n",
    "\n",
    "    return new_texts\n",
    "\n",
    "\n",
    "def generation_function(texts):\n",
    "    _inputs = texts if isinstance(texts, list) else [texts]\n",
    "    inputs = [prefix + inp for inp in _inputs]\n",
    "    inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"jax\"\n",
    "    )\n",
    "\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        **generation_kwargs\n",
    "    )\n",
    "    generated = output_ids.sequences\n",
    "    generated_recipe = target_postprocessing(\n",
    "        tokenizer.batch_decode(generated, skip_special_tokens=False),\n",
    "        special_tokens\n",
    "    )\n",
    "    return generated_recipe\n",
    "\n",
    "\n",
    "def get_recipe(items):\n",
    "    res = []\n",
    "    tmp = \"\"\n",
    "    generated = generation_function(items)\n",
    "    for text in generated:\n",
    "        sections = text.split(\"\\n\")\n",
    "        for section in sections:\n",
    "            section = section.strip()\n",
    "            if section.startswith(\"title:\"):\n",
    "                section = section.replace(\"title:\", \"\")\n",
    "                headline = \"TITLE\"\n",
    "            elif section.startswith(\"ingredients:\"):\n",
    "                section = section.replace(\"ingredients:\", \"\")\n",
    "                headline = \"INGREDIENTS\"\n",
    "            elif section.startswith(\"directions:\"):\n",
    "                section = section.replace(\"directions:\", \"\")\n",
    "                headline = \"DIRECTIONS\"\n",
    "\n",
    "            if headline == \"TITLE\":\n",
    "                tmp += f\"[{headline}]: {section.strip().capitalize()}\\n\"\n",
    "            else:\n",
    "                section_info = [\n",
    "                    f\"  - {i+1}: {info.strip().capitalize()}\" for i, info in enumerate(section.split(\"--\"))]\n",
    "                tmp += f\"[{headline}]:\\n\"\n",
    "                tmp += \"\\n\".join(section_info)\n",
    "        res.append(tmp)\n",
    "    return res\n",
    "\n",
    "\n",
    "# ========================================================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    items = [\n",
    "        \"macaroni, butter, salt, bacon, milk, flour, pepper, cream corn\",\n",
    "        \"provolone cheese, bacon, bread, ginger\"\n",
    "    ]\n",
    "    generated = generation_function(items)\n",
    "    for text in generated:\n",
    "        sections = text.split(\"\\n\")\n",
    "        for section in sections:\n",
    "            section = section.strip()\n",
    "            if section.startswith(\"title:\"):\n",
    "                section = section.replace(\"title:\", \"\")\n",
    "                headline = \"TITLE\"\n",
    "            elif section.startswith(\"ingredients:\"):\n",
    "                section = section.replace(\"ingredients:\", \"\")\n",
    "                headline = \"INGREDIENTS\"\n",
    "            elif section.startswith(\"directions:\"):\n",
    "                section = section.replace(\"directions:\", \"\")\n",
    "                headline = \"DIRECTIONS\"\n",
    "\n",
    "            if headline == \"TITLE\":\n",
    "                print(f\"[{headline}]: {section.strip().capitalize()}\")\n",
    "            else:\n",
    "                section_info = [\n",
    "                    f\"  - {i+1}: {info.strip().capitalize()}\" for i, info in enumerate(section.split(\"--\"))]\n",
    "                print(f\"[{headline}]:\")\n",
    "                print(\"\\n\".join(section_info))\n",
    "\n",
    "        print(\"-\" * 130)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-] Loaded configurations\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "import json\n",
    "\n",
    "\n",
    "# Importing config file\n",
    "CONFIG = None\n",
    "with open(\".\\\\..\\\\config.json\") as f:\n",
    "    CONFIG = json.load(f)\n",
    "print(\"[-] Loaded configurations\")\n",
    "\n",
    "client = QdrantClient(url=CONFIG[\"vector-db-url\"],\n",
    "                      api_key=CONFIG[\"vector-api-key\"])\n",
    "\n",
    "\n",
    "def searchProducts(query, allergens):\n",
    "    res = client.search(\n",
    "        collection_name=CONFIG[\"vector-collection-name\"],\n",
    "        query_filter=models.Filter(\n",
    "            must=[\n",
    "                models.FieldCondition(\n",
    "                    key=\"Food Product\",\n",
    "                    match=models.MatchValue(\n",
    "                        value=query,\n",
    "                    ),\n",
    "                ),\n",
    "                models.FieldCondition(\n",
    "                    key=\"Ingredients\",\n",
    "                    match=models.MatchAny(\n",
    "                        any=allergens,\n",
    "                        must_not=True  # This is the key change, indicating that the ingredients must not match any of the allergens\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "        search_params=models.SearchParams(hnsw_ef=128, exact=False),\n",
    "        query_vector=[0.2, 0.1, 0.9, 0.7],\n",
    "        # limit=3,\n",
    "        with_payload=[\"Food Product\", \"Ingredients\", \"Allergens\"],\n",
    "    )\n",
    "    return res\n",
    "\n",
    "\n",
    "def search_non_allergic_products(query, allergens):\n",
    "    out = client.scroll(\n",
    "        collection_name=CONFIG[\"vector-collection-name\"],\n",
    "        scroll_filter=models.Filter(\n",
    "            # should=[],\n",
    "            must=[\n",
    "                models.FieldCondition(\n",
    "                    key='metadata.\"Food Product\"', match=models.MatchText(text=query)),\n",
    "            ],\n",
    "            must_not=[\n",
    "                models.FieldCondition(key=\"metadata.Ingredients\", match=models.MatchText(text=allergen)) for allergen in allergens\n",
    "            ],\n",
    "        ),\n",
    "        with_payload=['metadata.\"Food Product\"',\n",
    "                      \"metadata.Ingredients\", \"metadata.Allergens\"],\n",
    "    )\n",
    "    res = []\n",
    "    for i in out[0]:\n",
    "        ele = i.payload[\"metadata\"]\n",
    "        if ele not in res:\n",
    "            res.append(ele)\n",
    "    return res\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(search_non_allergic_products(\"cookie\", [\"chocolate\", \"almond\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, jsonify, request\n",
    "import pandas as pd\n",
    "# from recipeBot import get_recipe\n",
    "# from search_better import search_non_allergic_products\n",
    "# from mistralBot import search_product\n",
    "\n",
    "data = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return \"This is backend\"\n",
    "\n",
    "\n",
    "@app.route('/search', methods=['POST'])\n",
    "def search():\n",
    "    data = request.get_json()\n",
    "    query = data[\"query\"]\n",
    "    allergens = data[\"allergens\"]\n",
    "    res = search_non_allergic_products(query, allergens)\n",
    "    return jsonify(res), 200\n",
    "\n",
    "\n",
    "@app.route(\"/recipe\", methods=[\"POST\"])\n",
    "def recipe():\n",
    "    data = request.get_json()\n",
    "    ingredients = data[\"ingredients\"]\n",
    "    res = get_recipe(ingredients)\n",
    "    return jsonify({\"recipes\": res}), 200\n",
    "\n",
    "\n",
    "@app.route(\"/chat\", methods=[\"POST\"])\n",
    "def chat():\n",
    "    data = request.get_json()\n",
    "    query = data[\"query\"]\n",
    "    allergies = data[\"allergies\"]\n",
    "    res = search_product(query, allergies)\n",
    "    return jsonify(res), 200\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host=\"0.0.0.0\", debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flask Subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['python', 'app.py']>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.Popen(['python','app.py'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Food Product                                        Ingredients  \\\n",
      "0       Almond Cookies                      Almonds, Sugar, Butter, Flour   \n",
      "1       Ranch Dressing    Buttermilk, Sugar, Vegetable oil, Garlic, herbs   \n",
      "2      Caramel Popcorn                       Popcorn, Sugar, Butter, Salt   \n",
      "3        Berry Parfait  Mixed berries, Sugar, Yogurt (milk, cultures),...   \n",
      "4          Mango Lassi    Mango, Sugar, Yogurt (milk, cultures), Cardamom   \n",
      "5       Banana Pudding  Bananas, Sugar, Milk, Vanilla pudding mix, coo...   \n",
      "6     Chocolate Mousse     Chocolate, Sugar, Heavy cream, Vanilla extract   \n",
      "7  Strawberry Smoothie                  Strawberries, Sugar, Yogurt, Milk   \n",
      "8        Berry Parfait                    Berries, Honey, Yogurt, Granola   \n",
      "9       Butter Chicken        Chicken, Sugar, Butter, Tomato sauce, cream   \n",
      "\n",
      "               Allergens  \n",
      "0  Almonds, Wheat, Dairy  \n",
      "1                  Dairy  \n",
      "2                  Dairy  \n",
      "3                  Dairy  \n",
      "4                  Dairy  \n",
      "5                  Dairy  \n",
      "6                  Dairy  \n",
      "7                  Dairy  \n",
      "8                  Dairy  \n",
      "9                  Dairy  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "print(df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
